{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072c57c-a9a9-4b2e-a742-d2950520d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-cudnn-cu11==8.6.0.163"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f059a-14c6-4a4a-b918-3742d9722b78",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b0073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44185e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "initialization failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mSystemError\u001b[0m: <built-in method __contains__ of dict object at 0x7fc7b2a127c0> returned a result with an error set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mai_experiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUtils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mai_experiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLockout\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mVAE\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VariationalAutoencoder, SupervisedVariationalAutoencoder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mai_experiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLockout\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mClassifiers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierModel, ClassifierModelXL, convnext_pretrained, transfer_learning\n",
      "File \u001b[0;32m/app/ai_experiments/Utils/utils.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msignalpet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msignalpet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_query\u001b[39m(list_im: \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n",
      "File \u001b[0;32m/app/app/learning/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLRFinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRFinder\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# import basic globals\u001b[39;00m\n",
      "File \u001b[0;32m/app/app/inference/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msignalpet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/pywrap_tf_session.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TF_SetTarget\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TF_SetConfig\n",
      "\u001b[0;31mImportError\u001b[0m: initialization failed"
     ]
    }
   ],
   "source": [
    "from ai_experiments.Utils import utils\n",
    "from ai_experiments.Lockout.helpers.VAE import VariationalAutoencoder, SupervisedVariationalAutoencoder\n",
    "from ai_experiments.Lockout.helpers.Classifiers import ClassifierModel, ClassifierModelXL, convnext_pretrained, transfer_learning\n",
    "from ai_experiments.Lockout.helpers.DataPreparation import CustomImageDataset, CustomImageDatasetLarge, create_data\n",
    "from ai_experiments.Lockout.helpers.Inference import inference_loop, inference_loop_cls, compute_metrics_cls_for_threshold\n",
    "from ai_experiments.Lockout.helpers.Train import train_loop_vae, train_loop_svae, train_loop_cls, train_loop_regression\n",
    "# from ai_experiments.Lockout.helpers.Utils import save_images_wandb\n",
    "import ai_experiments.Lockout.helpers.Clustering as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713bbed-6548-4b46-99ee-55b54eb48123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368639eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('/app/ai_experiments/Utils/sp_custom.mplstyle')\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from bson import json_util\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import ai_experiments.Utils.utils as utils\n",
    "from signalpet.images import extract_bboxes_from_organs\n",
    "import ast\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6042a7-cd2f-4492-a5d3-916a2c9a0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### torch.set_default_dtype(torch.float16)\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "class ConfigSettings:\n",
    "    def __init__(self):\n",
    "        self.TIME = current_time\n",
    "        self.MODEL = 'Bladder-stone'\n",
    "        self.EXP_NAME = f'{self.MODEL}_{current_time}'\n",
    "        self.beta = 1.5\n",
    "        self.alpha = 20\n",
    "        self.IMG_DIR = '/mnt/westside-tools/training_data/'\n",
    "        self.INPUT_DIM = 256\n",
    "        self.IN_CHANNELS = 1\n",
    "        self.NUM_CLS = 1\n",
    "        self.split = 0.8\n",
    "        self.filename = 'complete_df_info2'\n",
    "        self.Z_DIM = 16\n",
    "        self.PATCH_SIZE = 16,\n",
    "        self.D_DIM = 768,\n",
    "        self.NUM_EPOCHS = 100\n",
    "        self.LR_RATE = 3e-4\n",
    "        self.BATCH_SIZE = 32\n",
    "        \n",
    "        self.CLUSTERING = 'gm'\n",
    "        self.k_means_params = {\n",
    "            'n_clusters': 4,\n",
    "            'init': 'k-means++', \n",
    "            'max_iter': 1000, \n",
    "            'tol': 0.0001, \n",
    "            'verbose': 0, \n",
    "            'random_state': 42, \n",
    "            'copy_x': True, \n",
    "            'algorithm': 'lloyd'\n",
    "        }\n",
    "        \n",
    "CFG = ConfigSettings()\n",
    "\n",
    "# run = wandb.init(\n",
    "#   name= CFG.EXP_NAME, \n",
    "#   entity=\"lockout\",\n",
    "#   project=\"lockout-aina\",\n",
    "#   notes=\"train new model\",\n",
    "#   config=CFG,\n",
    "#   job_type=\"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ae775",
   "metadata": {},
   "source": [
    "# Create Dataset \n",
    "to train the model you nead a dataframe with the meta information we store in mongo\n",
    "\n",
    "\n",
    "you have two options:\n",
    "- create your own dataset\n",
    "- read from a .csv file (or train file) with the meta info \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc10ff7-d918-45e2-9387-5187f1627d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"bladder-bladder-stone\" #\"fracture-limb-fracture\"\n",
    "mask_name = \"bladder-stone-mask\" #'fracture-limb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80315c8",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "752dfbf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m res \u001b[38;5;241m=\u001b[39m Druid()\u001b[38;5;241m.\u001b[39mquery(q)\n\u001b[1;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(res)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_dataset(class_name, \u001b[38;5;28mlist\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "from signalpet.basics import *\n",
    "from signalpet.core import *\n",
    "# from app.learning import *\n",
    "# from app.core import *\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT model_name, image_id, value_from, value_to, suggestion_source, suggestion_value, ai_accuracy, modifiediou_score from\n",
    "    turing_queue_statistics\n",
    "    where done_at > ('2023-06-22')\n",
    "    and model_name = 'organ.%s'\n",
    "    and channel in ('qc', 'qc-offline')\n",
    "    and suggestion_source = 'ai'\n",
    "    and ai_accuracy > 0.85\n",
    "    and ai_accuracy < 1000\n",
    "\"\"\" % (mask_name)\n",
    "res = Druid().query(q)\n",
    "df = pd.DataFrame(res)\n",
    "\n",
    "utils.create_dataset(class_name, list(df['image_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAININGSET_DIR = \"/mnt/westside-tools/training_sets2/bladder-bladder-stone-20230914-134615\" # EXAMPLE:'/mnt/westside-tools/training_sets2/' #CHANGE THIS DIRECTORY WITH THE ONE FROM THE STEP ABOVE\n",
    "train = pd.read_pickle(os.path.join(TRAININGSET_DIR, \"train.pkl\"))\n",
    "valid = pd.read_pickle(os.path.join(TRAININGSET_DIR, \"valid.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions - from mongo, to have the segmentation outline which we need for this particular problem\n",
    "\n",
    "query_object_ids = utils.create_query(list(df['image_id']))\n",
    "mongo = Mongo().col('images')\n",
    "mongo_q = mongo.find(query_object_ids)\n",
    "\n",
    "df_mongo = pd.DataFrame()\n",
    "for row in mongo_q:\n",
    "\n",
    "    bbox = extract_bboxes_from_organs(row, [mask_name], sources=[\"results\"])[0]\n",
    "    df_temp = pd.DataFrame({'meta': json.dumps(row, default=json_util.default), \n",
    "                            'ai_labels': row['results']['organs'][mask_name], \n",
    "                            'ai_labels_bbox': bbox,\n",
    "                            '_id': row['_id']})\n",
    "    df_mongo = pd.concat([df_mongo, df_temp])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train._id = train._id.astype(str)\n",
    "df_mongo['_id'] = df_mongo['_id'].astype(str)\n",
    "combined_df = train.set_index('_id').merge(df_mongo.set_index('_id'), on='_id', how='inner')\n",
    "\n",
    "complete_df_info = combined_df.merge(df, left_on='_id', right_on='image_id', how='inner')\n",
    "\n",
    "# complete_df_info.to_csv(f'{class_name}_complete_df_info') # to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15919978-73a7-4487-ac3a-3afe077d18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193a08e-f2d4-4704-b6e9-99eefe472fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images\n",
    "for idx, row in complete_df_info.iterrows():\n",
    "\n",
    "    img = Image.open(f'{row[\"npyFilename\"]}')\n",
    "    (xmin, xmax, ymin, ymax) = ast.literal_eval(row[\"ai_labels_bbox\"])\n",
    "    # box - (left, upper, right, lower) int(x1), int(x2), int(y1), int(y2)\n",
    "    bbox = (xmin, ymin, xmax, ymax)\n",
    "    print(f'accuracy: {row[\"modifiediou_score\"]}')\n",
    "    \n",
    "    try:\n",
    "        crop_img = img.crop(bbox)\n",
    "        crop_img.save(f'{row[\"npyFilename\"].replace(\".jpg\", \"-cropped.jpg\")}')\n",
    "\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].imshow(img)\n",
    "        ax[1].imshow(crop_img)\n",
    "        plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error {e} with bbox: {bbox} \\nimage_id: {row[\"image_id\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f48113-a3ce-4a83-b837-9b53c3dd4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_df_info[['ai_accuracy', 'modifiediou_score']] = complete_df_info[['ai_accuracy', 'modifiediou_score']].applymap(lambda x: round(np.float64(x), 3))\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(x='ai_accuracy', y='modifiediou_score', data=complete_df_info)\n",
    "\n",
    "# plt.title('AI Accuracy vs Modified IoU Score')\n",
    "# plt.xlabel('AI Accuracy')\n",
    "# plt.ylabel('Modified IoU Score')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b1553",
   "metadata": {},
   "source": [
    "## Read df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filename = f'{class_name}_complete_df_info'\n",
    "complete_df_info = pd.read_csv(f\"{df_filename}\")\n",
    "CFG.NUM_IMG = len(complete_df_info)\n",
    "len(complete_df_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3925a5-6769-410a-a3b4-edd548cdddee",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82072d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_abnormal_df = complete_df_info[complete_df_info[\"ai_accuracy\"]>0.85]\n",
    "len(confident_abnormal_df)\n",
    "\n",
    "meta = confident_abnormal_df # -- when we want to train only on correct crops\n",
    "idx_train = np.round(CFG.NUM_IMG*CFG.split).astype(int)\n",
    "print(CFG.NUM_IMG, idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fa7cf-01a8-48e1-9e97-c6cf1a6344a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((CFG.INPUT_DIM, CFG.INPUT_DIM)),  \n",
    "    transforms.ToTensor(),  # Convert image to PyTorch Tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])  \n",
    "\n",
    "augmentations = A.Compose([\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 channel images\n",
    "\n",
    "train_dataset_a = CustomImageDataset(CFG.IMG_DIR, meta, 0, idx_train, transform=transform, augmentations=augmentations)\n",
    "train_loader_a = DataLoader(train_dataset_a, batch_size=32, shuffle=True)\n",
    "\n",
    "train_dataset = CustomImageDataset(CFG.IMG_DIR, meta, 0, idx_train, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = CustomImageDataset(CFG.IMG_DIR, meta, idx_train, CFG.NUM_IMG, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Channel images\n",
    "\n",
    "train_dataset_large_a = CustomImageDatasetLarge(CFG.IMG_DIR, meta, 0, idx_train, transform=transform, augmentations=augmentations)\n",
    "train_loader_large_a = DataLoader(train_dataset_large_a, batch_size=32, shuffle=False)\n",
    "\n",
    "train_dataset_large = CustomImageDatasetLarge(CFG.IMG_DIR, meta, 0, idx_train, transform=transform, augmentations=augmentations)\n",
    "train_loader_large = DataLoader(train_dataset_large, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset_large = CustomImageDatasetLarge(CFG.IMG_DIR, meta, idx_train, CFG.NUM_IMG, transform=transform)\n",
    "test_loader_large = DataLoader(test_dataset_large, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd806f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89dfae",
   "metadata": {},
   "source": [
    "# EMBED \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a922c4-b08a-4eb6-86a7-4d62ecdd7c22",
   "metadata": {},
   "source": [
    "##  Data for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e04e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the dataset - (n_samples, n_features), \n",
    "height = CFG.INPUT_DIM\n",
    "width = CFG.INPUT_DIM\n",
    "channels = 1\n",
    "\n",
    "X_train, y_true_train, y_true_train_binary = create_data(train_dataset)\n",
    "X_test, y_true_test, y_true_test_binary = create_data(test_dataset)\n",
    "\n",
    "X_train_img = X_train.reshape(-1, channels, CFG.INPUT_DIM, CFG.INPUT_DIM)\n",
    "X_test_img = X_test.reshape(-1, channels, CFG.INPUT_DIM, CFG.INPUT_DIM)\n",
    "\n",
    "test_image_ids = test_dataset.ids\n",
    "train_image_ids = train_dataset.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f0fcc-3109-4d88-b9e3-7c228d09295f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff18d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VariationalAutoencoder(CFG.INPUT_DIM, CFG.Z_DIM).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG.LR_RATE)\n",
    "loss_fn = nn.BCELoss(reduction=\"sum\")\n",
    "beta = CFG.beta\n",
    "\n",
    "model = train_loop_vae(model, optimizer, loss_fn, train_loader, CFG, DEVICE=DEVICE, wandb=wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f79723-54f0-4caf-b349-1d631f83dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/{CFG.EXP_NAME}.pth')\n",
    "wandb.log({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c462666-4b3f-41a5-a844-a92bc8e9bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained\n",
    "# gc.collect()\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VariationalAutoencoder(CFG.INPUT_DIM, CFG.Z_DIM).to(DEVICE)\n",
    "# model.load_state_dict(torch.load('xxx.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144f30e-ea83-46f7-8c0c-e10f255981f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c02f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_vae_train_recons, X_vae_train_z, y_vae_train = inference_loop(model, train_loader, DEVICE = DEVICE, v = True)\n",
    "X_vae_test_recons, X_vae_test_z, y_vae_test = inference_loop(model, test_loader, DEVICE = DEVICE, v = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18af37-e224-4690-824d-491ce65a1b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clustering_model, out_of_sample_pred = c.cluster_img(X_vae_train_z, \n",
    "                                                     y_vae_train, \n",
    "                                                     method='kmeans', \n",
    "                                                     test_data=X_vae_test_z, \n",
    "                                                     test_lbl=y_vae_test, \n",
    "                                                     **CFG.k_means_params)\n",
    "\n",
    "c.visualize_cluster(X_test_img, out_of_sample_pred, y_vae_test, num_images=8, name='embed')\n",
    "c.visualize_cluster_dist(out_of_sample_pred, y_vae_test, plot_t='kde', name='embed')\n",
    "\n",
    "y_vae_test_binary = np.where(y_vae_test>0.5,1, 0)\n",
    "stats = c.cluster_info(out_of_sample_pred, y_vae_test_binary, v = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc927a-97df-4e1f-b3e9-2b4e164e0704",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CLASSIFIER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511b5a8-6112-4753-8018-e3252d9814cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cls = ClassifierModel(CFG.INPUT_DIM).to(DEVICE)\n",
    "cls_optimizer = optim.Adam(cls.parameters(), lr=CFG.LR_RATE)\n",
    "cls_loss_fn = nn.BCELoss(reduction=\"sum\")\n",
    "cls = train_loop_cls(cls, cls_optimizer, cls_loss_fn, train_loader, CFG, DEVICE=DEVICE, wandb=wandb, verbose=True, th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d55464-762b-4809-ba09-4e41e26ab69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cls.state_dict(), f'models/{CFG.EXP_NAME}-classifier.pth')\n",
    "wandb.log({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c4529-fbe9-426b-b458-85c98408222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cls = ClassifierModel(CFG.INPUT_DIM).to(DEVICE)\n",
    "cls.load_state_dict(torch.load('models/Beta_20230814_193248_GM-classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbabbc-5b89-46c5-9e89-8c598229d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "X_test_cls_vec, y_pred_test_vec, y_test_cls_vec = inference_loop_cls(cls, test_loader, DEVICE = \"cuda\")\n",
    "X_train_cls_vec, y_pred_train_vec, y_train_cls_vec = inference_loop_cls(cls, train_loader, DEVICE = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ed7af-c8b9-4e49-8330-2db19c0fdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.k_means_params = {\n",
    "            'n_clusters': 10,\n",
    "            'init': 'k-means++', \n",
    "            'max_iter': 10000, \n",
    "            'tol': 0.00001, \n",
    "            'verbose': 0, \n",
    "            'random_state': 42, \n",
    "            'copy_x': True, \n",
    "            'algorithm': 'lloyd'\n",
    "        }\n",
    "\n",
    "CFG.gm_params = {'n_components': 10,\n",
    "     'covariance_type': 'full',\n",
    "     'tol': 0.0001,\n",
    "     'reg_covar': 1e-06,\n",
    "     'max_iter': 1000,\n",
    "     'n_init': 1,\n",
    "     'init_params': 'kmeans',\n",
    "     'weights_init': None,\n",
    "     'means_init': None,\n",
    "     'precisions_init': None,\n",
    "     'random_state': 42}\n",
    "\n",
    "wandb.log(CFG.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e76370-798e-4310-99f6-2b3813348c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cluster embeddings\n",
    "clustering_model, out_of_sample_pred = c.cluster_img(X_train_cls_vec, \n",
    "                                                     y_train_cls_vec, \n",
    "                                                     method='gm', \n",
    "                                                     test_data = X_test_cls_vec,\n",
    "                                                     test_lbl = y_test_cls_vec,\n",
    "                                                     **CFG.gm_params)\n",
    "\n",
    "c.visualize_cluster(X_test_img, out_of_sample_pred, y_test_cls_vec, num_images=3, name='embed')\n",
    "c.visualize_cluster_dist(out_of_sample_pred, y_test_cls_vec, plot_t='kde', name='embed')\n",
    "\n",
    "save_images_wandb(wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db16551-2ca4-4645-97a9-cad5931efe0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_cls_test_binary = np.where(y_test_cls_vec>0.5,1, 0)\n",
    "stats = c.cluster_info(out_of_sample_pred, y_cls_test_binary, y_test_cls_vec, v = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424dbe21-e2a3-47df-99a7-207833021820",
   "metadata": {},
   "source": [
    "# ConvNextXL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d85c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1 CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cceba49-f1ed-4a72-a7f5-b1aa5f60f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = convnext_pretrained(in_chans=1, num_classes=CFG.NUM_CLS, depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72385a-ba5d-44d5-9273-52d504558779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), f'models/{CFG.EXP_NAME}-pretrainedConvNeXt.pth')\n",
    "# wandb.log({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0220e9-f115-4f7b-a173-8cf2df1092af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fine Tune\n",
    "torch.cuda.empty_cache()\n",
    "CFG.NUM_EPOCHS = 10\n",
    "CFG.LR_RATE = 0.00001\n",
    "\n",
    "wandb.log(CFG.__dict__)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG.LR_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = train_loop_cls(model, optimizer, loss_fn, train_loader_a, CFG, DEVICE=DEVICE, wandb=wandb, verbose=False, th=0.5, use_clip_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4dc01-9f39-4f4a-953e-57ea5ec2e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/{CFG.EXP_NAME}-ConvNeXt.pth')\n",
    "wandb.log({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8a3dec-338f-4f8e-b661-02f8c3e4a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = convnext_pretrained(in_chans=1, num_classes=CFG.NUM_CLS, depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048]).to(DEVICE)\n",
    "# model.load_state_dict(torch.load('models/convnext_20230815_233251-ConvNeXt.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905f638-375d-4b10-8101-3c82ee302dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ConvNEXT_vec, y_pred_train_vec, y_train_ConvNEXT_vec = inference_loop_cls(model, train_loader, DEVICE = DEVICE)\n",
    "X_test_ConvNEXT_vec, y_pred_test_vec, y_test_ConvNEXT_vec = inference_loop_cls(model, test_loader, DEVICE = DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aba2a4-7296-4d46-ab0f-e5ed40c2f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics_cls_for_threshold(y_train_ConvNEXT_vec, y_pred_train_vec, threshold=0.5)\n",
    "compute_metrics_cls_for_threshold(y_test_ConvNEXT_vec, y_pred_test_vec, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed913dea-6cc6-48d3-98fe-8acc0b792a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cluster embeeddings\n",
    "CFG.k_means_params = {\n",
    "            'n_clusters': 6,\n",
    "            'init': 'k-means++', \n",
    "            'max_iter': 10000, \n",
    "            'tol': 0.00001, \n",
    "            'verbose': 0, \n",
    "            'random_state': 42, \n",
    "            'copy_x': True, \n",
    "            'algorithm': 'lloyd'\n",
    "        }\n",
    "\n",
    "clustering_model, out_of_sample_pred = c.cluster_img(X_train_ConvNEXT_vec, \n",
    "                                                     y_train_ConvNEXT_vec, \n",
    "                                                     method='kmeans', \n",
    "                                                     test_data = X_test_ConvNEXT_vec,\n",
    "                                                     test_lbl = y_test_ConvNEXT_vec,\n",
    "                                                     **CFG.k_means_params)\n",
    "\n",
    "c.visualize_cluster(X_test_img, out_of_sample_pred, y_test_ConvNEXT_vec, num_images=8, name='embed', display_all=False)\n",
    "c.visualize_cluster_dist(out_of_sample_pred, y_test_ConvNEXT_vec, plot_t='kde', name='embed')\n",
    "y_ConvNEXT_test_binary = np.where(y_test_ConvNEXT_vec>0.5,1, 0)\n",
    "stats = c.cluster_info(out_of_sample_pred, y_ConvNEXT_test_binary, y_test_ConvNEXT_vec, v = True)\n",
    "\n",
    "save_images_wandb(wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f005864",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualization tools for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c2392-6261-445a-b696-5c117791bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = c.cluster_info(out_of_sample_pred, y_ConvNEXT_test_binary, y_test_ConvNEXT_vec, v = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296ce32-807f-43bf-abcf-3584ef9886af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.visualize_cluster(X_test_img, out_of_sample_pred, y_test_ConvNEXT_vec, num_images=8, name='embed', display_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78df7c3-7598-4484-8bbf-fb6db1695eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.ss_cluster(X_test_img, labeled_data_test, test_image_ids, y_test_ConvNEXT_vec, out_of_sample_pred, num_images=8, name='cls3ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef8614-f656-4d66-ba9e-6fdbf25ef364",
   "metadata": {},
   "source": [
    "## ConvNexT 3 CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a4d88-a46b-40ef-9afe-b6b28474a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = convnext_pretrained(in_chans=3, num_classes=CFG.NUM_CLS, depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce329c69-70ed-4921-a1f4-64c218c170b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbbc83-9ccd-4460-b2cb-98e0b8975461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fine Tune\n",
    "torch.cuda.empty_cache()\n",
    "CFG.NUM_EPOCHS = 10 #10 \n",
    "CFG.LR_RATE = 0.0001 #perfect - 0.00001, slow\n",
    "\n",
    "# wandb.log(CFG.__dict__)\n",
    "model.train()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG.LR_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = train_loop_cls(model, optimizer, loss_fn, train_loader_large_a, CFG, DEVICE=DEVICE, verbose=False, th=0.5, use_clip_grad=True)\n",
    "\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), f'{CFG.EXP_NAME}-BladderC3-12.pth')\n",
    "# wandb.log({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73be3c-43bc-40ad-8aab-62e30f87cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ConvNEXT_vec, y_pred_train_vec, y_train_ConvNEXT_vec = inference_loop_cls(model, train_loader_large, DEVICE = DEVICE)\n",
    "X_test_ConvNEXT_vec, y_pred_test_vec, y_test_ConvNEXT_vec = inference_loop_cls(model, test_loader_large, DEVICE = DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1195086-976c-409d-a46b-4f936ef3f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ConvNEXT_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb96f99-79ee-478c-922a-6177e908d850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cluster embeeddings\n",
    "CFG.k_means_params = {\n",
    "            'n_clusters': 1,\n",
    "            'init': 'k-means++', \n",
    "            'max_iter': 10000, \n",
    "            'tol': 0.00001, \n",
    "            'verbose': 0, \n",
    "            'random_state': 42, \n",
    "            'copy_x': True, \n",
    "            'algorithm': 'lloyd'\n",
    "        }\n",
    "\n",
    "clustering_model, out_of_sample_pred = c.cluster_img(X_train_ConvNEXT_vec, \n",
    "                                                     y_train_ConvNEXT_vec, \n",
    "                                                     method='kmeans', \n",
    "                                                     test_data = X_test_ConvNEXT_vec,\n",
    "                                                     test_lbl = y_test_ConvNEXT_vec,\n",
    "                                                     **CFG.k_means_params)\n",
    "\n",
    "c.visualize_cluster(X_test_img, out_of_sample_pred, y_test_ConvNEXT_vec, num_images=8, name='embed', display_all=False)\n",
    "c.visualize_cluster_dist(out_of_sample_pred, y_test_ConvNEXT_vec, plot_t='kde', name='embed')\n",
    "y_ConvNEXT_test_binary = np.where(y_test_ConvNEXT_vec>0.5,1, 0)\n",
    "stats = c.cluster_info(out_of_sample_pred, y_ConvNEXT_test_binary, y_test_ConvNEXT_vec, v = True)\n",
    "\n",
    "# save_images_wandb(wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f39d1-fe1b-4784-8930-c3dd8b60d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.visualize_cluster(X_test_img, out_of_sample_pred, y_test_ConvNEXT_vec, num_images=8, name='embed', display_all=False)\n",
    "c.visualize_cluster_dist(out_of_sample_pred, y_test_ConvNEXT_vec, plot_t='kde', name='embed')\n",
    "y_ConvNEXT_test_binary = np.where(y_test_ConvNEXT_vec>0.5,1, 0)\n",
    "stats = c.cluster_info(out_of_sample_pred, y_ConvNEXT_test_binary, y_test_ConvNEXT_vec, v = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e56df-3e85-46ad-8a4b-c3ccb1f77721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.visualize_cluster(X_test_img, out_of_sample_pred, y_test_ConvNEXT_vec, num_images=8, name='embed', display_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74071593-9296-47b6-9f71-c2dd3e72ab7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# c.ss_cluster(X_test_img, labeled_data_test, test_image_ids, y_test_ConvNEXT_vec, out_of_sample_pred, num_images=8, name='cls3ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcdb2e4",
   "metadata": {},
   "source": [
    "# EXPORT TO ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e0918",
   "metadata": {},
   "source": [
    "### CLUSTERING - SKLEARN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81176580",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import skl2onnx\n",
    "\n",
    "clustering_model = 'insert your model' - load it\n",
    "\n",
    "initial_type = [('inputs', skl2onnx.common.data_types.FloatTensorType([None, 2048]))]  # Replace 2 with the number of features in your actual model\n",
    "\n",
    "Convert the scikit-learn model to ONNX format\n",
    "onnx_model = skl2onnx.convert_sklearn(clustering_model, initial_types=initial_type)\n",
    "\n",
    "# Save the ONNX model\n",
    "with open(f\"models/{CFG.EXP_NAME}-clustering_model.onnx\", \"wb\") as f:\n",
    "     f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7fc476",
   "metadata": {},
   "source": [
    "### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6110f0",
   "metadata": {},
   "source": [
    "import torch.onnx\n",
    "\n",
    "dynamic_axes = {'inputs': {0: 'batch_size'},  # indicating the dimensions that are dynamic\n",
    "                'outputs': {0: 'batch_size'}}\n",
    "\n",
    "x = torch.randn(32, 3, 256, 256).to(DEVICE) #B,C,H,W\n",
    "torch.onnx.export(model, x, \"model.onnx\", verbose=False, input_names=['inputs'], output_names=['outputs'], dynamic_axes=dynamic_axes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
